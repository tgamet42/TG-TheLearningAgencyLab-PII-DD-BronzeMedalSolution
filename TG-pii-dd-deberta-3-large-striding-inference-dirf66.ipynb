{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4bb0f3",
   "metadata": {
    "papermill": {
     "duration": 0.0067,
     "end_time": "2024-04-20T04:27:38.189379",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.182679",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸŸï¸ Credits\n",
    "\n",
    "Main Citation:\n",
    "@misc{pii-detection-removal-from-educational-data,\n",
    "    author = {Langdon Holmes, Scott Crossley, Perpetual Baffour, Jules King, Lauryn Burleigh, Maggie Demkin, Ryan Holbrook, Walter Reade, Addison Howard},\n",
    "    title = {The Learning Agency Lab - PII Data Detection},\n",
    "    publisher = {Kaggle},\n",
    "    year = {2024},\n",
    "    url = {https://kaggle.com/competitions/pii-detection-removal-from-educational-data}\n",
    "}\n",
    "\n",
    "- Stride is something Raja first shared: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/473011\n",
    "- The biggest booster in performance is changing triplets to pairs in token reassembling: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854/comments#2659393\n",
    "\n",
    "Updates and improvements made by Thomas Gamet\n",
    "This notebook is under the MIT License.\n",
    "Will test out of striding on inference alone helps my deberta-v3-large trained model\n",
    "This came from: https://www.kaggle.com/code/valentinwerner/945-deberta-3-base-striding-inference\n",
    "\n",
    "This particular notebook was forked from a prior version of mine when I realized that it was using more than 1/3rd the memory available to complete a leader board submission. This notebook was rewritten to write its submission file by immediately reassembling inferences and writing them to the submission csv file. The first memory efficiency rewrite seems to have heavy dictionary and Dataset initialization, but it does not appear to that it makes a significant performance difference.\n",
    "\n",
    "April 2024 fork.\n",
    "This is a fork off of v66 of https://www.kaggle.com/code/thomasgamet/pii-dd-deberta-3-large-striding-inference-direct made for testing threshold settings and different base scores of the sample_submission.csv.\n",
    "\n",
    "Origin story:\n",
    "See below: \n",
    "v66 Ensemble test tv12&trn2-v11 with trshold 0.99, maxlen=2048, -> yielding LB=0.968\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v11\n",
    "Where https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-training is version 12 with build parameters ep5lr1e-5ds2nfr12seed421td.01 which used a single inference max length of 720 and a batch size of 2.\n",
    "Model-trn2-v11 is saved in dataset https://www.kaggle.com/datasets/thomasgamet/piidd-extending-large-model-starter-model-trn2\n",
    "  This model is built in https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v10 built with v10ep4lr1e-5ds2nfr12seed8128td.1P100chn720bat4 which reads all documents in a chunk size of 720 and uses a batch size of 4.\n",
    "  It is also stored in https://www.kaggle.com/datasets/thomasgamet/piidd-extending-large-model-starter-best2-trn2\n",
    "  \n",
    "Original was 28 PII predictions on threshold 0.99 and sample_submission 0.9984 with LB=0.968\n",
    "First run, with threshold of 0.9977 with 34 PII predictions has a sample_submission score of 0.9433 produced a public LB score of 0.960 (< 0.968).\n",
    "Second run, with threshold of 0.9925 with 30 PII predictions has a sample_submission score of 0.9677 produced a public LB score of 0.969 (seems like a small improvement).\n",
    "Third run, with threshold of 0.9951 with 31 PII predictions has a sample_submission score of 0.9615 produced a public LB score of 0.968\n",
    "Fourth run, with threshold of 0.99125 with 29 PII predictions has a sample_submission score of 0.9740 produced a public LB score of 0.967.\n",
    "Fifth run, with threshold of 0.98 with 27 PII predictions has a sample_submission score of 0.9970 produced a public LB score of 0.970.\n",
    "Sixth run, with threshold of 0.985 with 27 PII predictions has a sample_submission score of 0.9970 produced a public LB score of 0.969.\n",
    "\n",
    "This model is now selected to produce the three contest entries. I am going to test the thresholding effect and then pick the highest scoring option and one both with a slightly increased and a slightly decreased threshold. The question I have is what is the likely variance in an ideal threshold when moving to the 3x private test data. \n",
    "\n",
    "The metrics so far:\n",
    "\n",
    "Threshold    Public LB   Sample estLB Positives in Sample   Experiment\n",
    "   0.9977        0.960         0.9883                  34   v1\n",
    "   0.9951        0.968         0.9927                  31   v3\n",
    "   0.9925        0.969         0.9941                  30   v2\n",
    "   0.99125       0.967         0.9956                  29   v4\n",
    "   0.9900        0.968         0.9971                  28   Original v66\n",
    "   0.9850        0.969         0.9985                  27   v6\n",
    "   0.9800        0.970         0.9985                  27   v5 \n",
    "   0.9750        0.969         0.9985                  27   v7\n",
    "   0.9700        0.969         0.9995                  27   v8\n",
    "   0.9600        0.970         1.0000                  26   v9 *.6x.4x0.970 .4x.6x0.970\n",
    "   0.9575        0.970         1.0000                  26   v14\n",
    "   0.9550        0.970         1.0000                  26   v12 \n",
    "   0.9525        0.970         1.0000                  26   v16\n",
    "   0.9500        0.970         1.0000                  26   v10\n",
    "   0.9450        0.970         1.0000                  26   v17\n",
    "   0.9425        0.971         1.0000                  26   V26\n",
    "   0.9412        0.971         1.0000                  26   v27 *.6x.4x0.970 .4x.6x0.970\n",
    "   0.9400        0.971         1.0000                  26   v18\n",
    "   0.9375        0.970         1.0000                  26   v28\n",
    "   0.9350        0.970         1.0000                  26   v19\n",
    "   0.9300        0.970         1.0000                  26   v20 *.6x.4x0.969 .4x.6x0.970\n",
    "   0.9250        0.969         1.0000                  26   v13 \n",
    "   0.9200        0.968         1.0000                  26   v21\n",
    "   0.9150        0.968         1.0000                  26   v22\n",
    "   0.9100        0.967         1.0000                  26   v23\n",
    "   0.9050        0.966         1.0000                  26   v24\n",
    "   0.9000        0.965         1.0000                  26   v11\n",
    "   0.8500        0.963         1.0000                  26   v15\n",
    "   0.8000        0.953         1.0000                  26   v25\n",
    "\n",
    "Obervations based upon these results and prior experiences.\n",
    "a. The sample submission and public submission both achive their best scores at the highest threshold for their individual scores at a threshold of 0.9400\n",
    "b. The sample submission score drops only with false positives increasing at thresholds above 0.9400 to 0.9600 tested first at 0.9700, however the public LB score fluctuates still high indicating a combination of more false negatives being converted to true positives at a rate that sustains the score as false positives rise until the threshold of 0.9951 is exceeded.\n",
    "c. The sample submission is not useful for detecting a drop in true positives (and thus rise in false negatives) below a threshold of 0.9400. The public leaderboard however shows for a maximum score from thresholds of 0.9600 down to 0.9400, and stability down to a threshold as low as 0.9100.\n",
    "d. If the public set of 25% of the test data is a well represented portion of the total test set, then the other 75% will produce a similar result. However, this is likely not the case as the larger dataset is likely to contain some outliers neither training covered,or the smaller dataset pre-validated for establishing the threshold for the best score. As a result, only the first ensemble threshold picked will be v27 with a public LB of 0.971 from a threshold of 0.9412. This may just be an overfit point,though it also seems plausible that a best case occurs when the false negatives to true positive conversion rate peaks the near highest score is likely reached. My conjecture is that the tipping point is not on a bell curve, but rather skewed in favor of the best score being on the lower end of thresholds.\n",
    "e. The model is stable for a threshold of up to 0.9950 and down to 0.9100. My goal is to cover maximizing the stable region of thresholds so I'll select the other two entries at the top and bottom of the 0.970 LB scores.\n",
    "f. The second model select is v9 at a threshold of 0.9600.\n",
    "G. The third model selected is what is detected as the bottom (lowest threshold) that still produced a 0.970 high score in the samples taken. This is at version 20 with threshold 0.9300.\n",
    "h. Each of the selected models was run with a .6 and .4 weight distribution for each of the two models. All 3 pairs of tests showed a stable outcome in each cases, with an even split (.5 each) doing as well or slightly better than either imbalance. This gives me a bit more confidence that the models are stable, though that is just a conjecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3ed68",
   "metadata": {
    "papermill": {
     "duration": 0.00609,
     "end_time": "2024-04-20T04:27:38.202025",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.195935",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Restructured to write predictions immediately to the submission file\n",
    "\n",
    "With the private dataset expected to be 3 times larger than the public dataset I suspect I need to get the code to run in a third of the free space memory structure. It now seems I need to avoid building up all predictions and the ds_dict as 25% of available RAM is insufficient to maintain all of these structures. It's going to be a bit \"ugly\" but not really complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ae3a0",
   "metadata": {
    "papermill": {
     "duration": 0.006097,
     "end_time": "2024-04-20T04:27:38.214222",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.208125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the threshold at 0.99.\n",
    "The deberta-v3-large using 12 of 24 frozen layers, lr=1e-5, 2 data sets, and 2 epochs it scored an LB=0.956\n",
    "The deberta-v3-large using 6 of 24 frozen layers, lr=5e-6, 2 data sets, and 4 epochs it scored an LB=0.952\n",
    "\n",
    "Multiple tests were run with ep2lr1e-5ds2fr12seed421+.01testsplit\n",
    "v4 threshold 0.99 LB=0.941\n",
    "v5 threshold 0.97 LB=0.947\n",
    "v6 threshold 0.90 LB=0.947\n",
    "v7 threshold 0.98 LB=0.946\n",
    "v8 threshold 0.94 LB=0.948\n",
    "It seems definitive two epochs with 99% of the training data is not producing good results. Best bet is to try several thresholds with 1 and 3 epochs of training aiming for the LB scores to be like a cross validation score. If that does not work well enough then the full training may require smaller learning rates to do well, or more frozen layers, which is to be determined.\n",
    "\n",
    "Loading Dataset v6 We now have 3 epochs with ep2lr1e-5ds2fr12seed421+.01td\n",
    "/kaggle/input/piidd-extending-large-model-starter-model/output/\n",
    "v9 threshold 0.99 LB=0.920 (checkpoint-4535)\n",
    "v10 threshold 0.99 LB=0.958 (checkpoint-13605)\n",
    "v11 threshold 0.97 LB=0.949 (checkpoint-13605)\n",
    "v12 threshold dynamic LB=0.953 (checkpoint-13605) - highest PII confidence -0.0075\n",
    "v13 threshold dynamic LB=0.952 (checkpoint-13605) - highest PII confidence -0.0100\n",
    "New strategy => pick the model epoch that gives the best LB for a threshold of .99\n",
    "\n",
    "Loaded Dataet v7 /kaggle/input/piidd-extending-large-model-starter-\n",
    "ep4lr1e-5ds3fr12seed421+.01td epochs 4\n",
    "model/output/checkpoint-26920\n",
    "v14 threshold 0.99 LB=0.932\n",
    "model/output/checkpoint-20190\n",
    "v15 error running without GPU, need to wait for it to faill\n",
    "v16 threshold 0.99 LB=0.940\n",
    "model/output/checkpoint-13460\n",
    "v17 threshold 0.99 LB=0.951\n",
    "model/output/checkpoint-6730\n",
    "v18 threshold 0.99 LB=0.930\n",
    "\n",
    "ep#lr2e-6ds3fr12seed42+.1td-of5ep\n",
    "Loaded Dataet v8 /kaggle/input/piidd-extending-large-model-starter-\n",
    "model/output/checkpoint-6118\n",
    "v19 threshold 0.99 LB=0.924\n",
    "model/output/checkpoint-12236\n",
    "v20 threshold 0.99 LB=0.945\n",
    "model/output/checkpoint-18354\n",
    "v21 threshold 0.99 LB=0.933\n",
    "model/output/checkpoint-24472\n",
    "v22 threshold 0.99 LB=0.935\n",
    "model/output/checkpoint-30590\n",
    "v23 was not a submission\n",
    "v24 threshold 0.99 LB=0.934\n",
    "\n",
    "ep#lr2e-5ds3fr12seed42+.1td+of3ep\n",
    "Loaded Dataset v9 /kaggle/input/piidd-extending-large-model-starter-\n",
    "model/output/checkpoint-6118\n",
    "v25 threshold 0.99 LB=0.942\n",
    "model/output/checkpoint-12236\n",
    "v26 threshold 0.99 LB=0.948\n",
    "model/output/checkpoint-18354\n",
    "v27 threshold 0.99 LB=0.945\n",
    "\n",
    "Loaded Dataset v10 \n",
    "ep#lr5e-6ds4fr12seed42td.1tb4vb8\n",
    "checkpoint-10527 which is ep3\n",
    "v28 threshold 0.99 LB=0.939\n",
    "checkpoint-14036 which is ep4\n",
    "v29 threshold 0.99 LB=0.942\n",
    "\n",
    "Trained v11 v11 ep4lr9e-6ds4fr18seed42td.1tb4vb8\n",
    "checkpoint-14036 which is ep4 with the highest training F1 score\n",
    "v30 threshold 0.99 LB=0.938\n",
    "\n",
    "Also, an experiment or two with 2048 and 720 for INFERENCE_MAX_LENGTH is needed.\n",
    "v34 using training v11 ep3lr1e-5ds2nfr12seed421td.1 threshold 0.99\n",
    "LB at maxlen=2048 was 0.953\n",
    "LB at maxlen=720 was 0.951\n",
    "LB at maxlen=1024 was 0.952\n",
    "\n",
    "v35 using training v12 ep5lr1e-5ds2nfr12seed421td.01 threashold 0.99\n",
    "LB at maxlen=1024 with epoch 4 of 5 was 0.947\n",
    "v36 LB at maxlen=1024 with epoch 3 of 5 was 0.948\n",
    "v37 LB at maxlen=1024 with epoch 5 of 5 was 0.949\n",
    "v38 LB at maxlen=1024 with epoch 2 of 5 was 0.958\n",
    "v39 LB at maxlen=1024 with epoch 1 of 5 was 0.963 (short lived winner)\n",
    "v40 LB at maxlen=2048 with epoch 1 of 5 was 0.965 (new winner)\n",
    "\n",
    "The aim here is a similar LB with 3 different training datasets to see if they can form an ensemble. Theory, training with all datasets together decreases generalization. They high learning rate, compared to what the model specs say it was trained at, are likely the reason why it both 'learns' in an epoch or two, and why it soon starts loosing performance (overfits on its training data and loses generalization).\n",
    "Training v22 ep2lr1e-5ds3nfr12seed421td.01\n",
    "Concern, I have to basically negate the threshold to 0.001 to get it to filter down answers.\n",
    "v41 LB at maxlen=2048 with epoch 1 of 2 was 0.906\n",
    "v42 LB at maxlen=2048 with epoch 2 of 2 was 0.915\n",
    "This dataset is not promissing.\n",
    "\n",
    "Trying with v14 in the model dataset v23ep3lr1e-5ds2nfr12seed421td.01\n",
    "v43 LB at maxlen-2048 with epoch 1 of 3 was 0.934\n",
    "v44 LB at maxlen-2048 with epoch 2 of 3 was 0.948\n",
    "v45 LB at maxlen-2048 with epoch 3 of 3 was 0.954\n",
    "We need more epochs to see if at which one it reaches the point of an overfit.\n",
    "v46 LB at maxlen-2048 with epoch 4 of 6 was 0.941 which shows we have begun overfitting\n",
    "\n",
    "Next move, rebuild v12 with slight alterations to see if it helps in an ensemble.\n",
    "\n",
    "v46 ran with an error - wrong model checkpoint selection\n",
    "v47 confirming we rerun v40 with Trained model v12 /kaggle/input/piidd-extending-large-model-starter-model\n",
    "v48 seeing if v1 of trn2-model is like v12 above (two more epochs to try) -> LB=0.94\n",
    "That is different, but there are two more epochs to test as if an LB was a CV.\n",
    "v49 with v1 epoch 2 -> LB=0.958\n",
    "v50 with v1 epoch 3 -> LB=? (mislabeled run, it really is epoch 3's model output). \n",
    "\n",
    "**** New Direct CSV generation on a per document's prediction.\n",
    "\n",
    "v1 LB=0.963 recreating Training v12 ep5lr1e-5ds2nfr12seed421td.01 threashold 0.99 results but with 16GB reserved for the private dataset size expansion on a max length 1024 tokens\n",
    "v2 LB-0.965 provided all of V1 works but with 2048 allowed. It is also good to note that I believe the memory footprint is now much lower so if this works OOM should no longer be a major concern.\n",
    "v3 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048 for the competition at an LB=0.965 with full RAM available.\n",
    "v4 builds on trn2 v02-ep1of4lr1e-5ds2nfr12seed9973tr.01, threshold 0.99, maxlen-2048 for competition at an LB=0.952 with full RAM and epoch 0.\n",
    "v5 builds on trn2 trn2v2-ep2of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.945 with full RAM and epoch 1.\n",
    "v6 builds on trn2 trn2v2-ep3of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.959 with full RAM and epoch 2.\n",
    "v7 builds on trn2 trn2v2-ep4of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.947 with full RAM and epoch 2.\n",
    "\n",
    "v8 Ensemble test yielding LB=0.967 new high\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v2-ep3of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "Also, this run will remove 20GB of RAM to confirm more than enough memory exists.\n",
    "v9 is the same ensemble with all memory available.\n",
    "\n",
    "v10 Ensemble test yielding LB=0.968 new high\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v1-ep3of3lr1e-5ds2nfr12seed4221td.01, threshold 0.99, maxlen-2048\n",
    "  \n",
    "Single model run with trn2v3-ep4lr1e-5ds2nfr12seed8128td.01\n",
    "v11 with Epoch 1 produces LB=0.910\n",
    "v12 with Epoch 2 produces LB=0.962\n",
    "v13 with Epoch 3 produces LB=0.952\n",
    "\n",
    "v14 Ensemble test yielding LB=0.967\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v3-ep2of3lr1e-5ds2nfr12seed8128td.01, threshold 0.99, maxlen=2048\n",
    "  \n",
    "v15 Ensemble test for memory footprint yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v2-ep3of3lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "  c. trn2 trn2v2-ep1of3lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "It finished in 2.75 hours with 18GB of memory reserved for the private set size.\n",
    "The LB=0.965 (which is fine as the individual models had 0.965, 0.959, 0.952).\n",
    "Though this will be close with the private data size being 3x the public data size I will try at least one three model run maximizing the LB. The other two selections will use good, but not optimal, LBs based upon multiple prior experiences where the mid to upper mid CV's tend to produce the best LB scores, and I'm pretty much treating the LB like a CV score.\n",
    "\n",
    "The [Best-Trn2] contains:\n",
    "v1 is ep3lr1e-5ds2nfr12seed4221td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v1] epoch 3 LB=0.961\n",
    "v2 is ep4lr1e-5ds2nfr12seed9973td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v2] epoch 3 LB=0.959\n",
    "v3 is ep4lr1e-5ds2nfr12seed8128td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v3] epoch 2 LB=0.962 which is Trn2 v3\n",
    "\n",
    "v16 was an invalid build attempt with the original model not correctly pinned.\n",
    "\n",
    "v17 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v1\n",
    "  c. Model-trn2-v2\n",
    "  \n",
    "v18 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v1\n",
    "  c. Model-trn2-v3\n",
    "  \n",
    "v19 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v2\n",
    "  c. Model-trn2-v3\n",
    "  \n",
    "v20 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.961\n",
    "  a. Model-trn2-v1\n",
    "  b. Model-trn2-v2\n",
    "  c. Model-trn2-v3\n",
    "\n",
    "Well, this was disappointing, a three model ensemble failed to produce an improved score, but with that said, nont all models have equal weight. A few more tries with weights on v18 which combines the top three individual models.\n",
    "\n",
    "v21 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. 0.4 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.3 Model-trn2-v1\n",
    "  c. 0.3 Model-trn2-v3\n",
    "  \n",
    "v22 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.25 Model-trn2-v1\n",
    "  c. 0.25 Model-trn2-v3\n",
    "  \n",
    "v23 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.2 Model-trn2-v1\n",
    "  c. 0.3 Model-trn2-v3\n",
    "  \n",
    "v24 was v23 with maxlen on inference set to 4096 and testing for OOM. It ran to completion, and on time, but scored only 0.969.\n",
    "\n",
    "v25 Ensemble test with trshold 0.99, maxlen=4096, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=4096\n",
    "  b. 0.25 Model-trn2-v1\n",
    "  c. 0.25 Model-trn2-v3\n",
    "  \n",
    "Left weights at .5 .25 .25\n",
    "v26 Ensemble test with trshold 0.970, maxlen=2048, -> yielding LB=0.967\n",
    "v27 Ensemble test with trshold 0.995, maxlen=2048, -> yielding LB=0.967\n",
    "\n",
    "v28 Ensemble test with trshold 0.970, maxlen=2048, w.34.33.33 -> yielding LB=0.967\n",
    "v29 Ensemble test with trshold 0.995, maxlen=2048, w.34.33.33 -> yielding LB=0.967\n",
    "\n",
    "OK, it seems we are stuck and for the LB a threshold of 0.99 has proven the most effective.\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v4) until it could fine tune with TRAINING_MAX_LENGTH = 1024 (previously OOM occurred at values over 720). Let's see this matchup to the hidden layer size improves the models produced  to yield better inferencing results.\n",
    "\n",
    "ep4lr1e-5ds2nfr12seed8128td.01GPUP100tml1k as an \"ensemble\" of one.... my mistake\n",
    "v30 Ensemble test with checkpoint-4702 thrshold 0.99, maxlen=2048, -> yielding LB=0.923\n",
    "v31 Ensemble test with checkpoint-9404 thrshold 0.99, maxlen=2048, -> yielding LB=0.956\n",
    "v32 Ensemble test with checkpoint-14106 thrshold 0.99, maxlen=2048, -> yielding LB=0.955\n",
    "v33 Ensemble test with checkpoint-18808 thrshold 0.99, maxlen=2048, -> yielding LB=0.954\n",
    "\n",
    "v34 Ensemble test with checkpoint-9404 threshold 0.90, maxlen=2048 -> yielding LB=0.948\n",
    "Reducing the threshold increases the nummber of tokens seen as class \"O\" (not PII) which means positive PII detection decreases, and this lowers the score even though recall is weights over precision. \n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v5) until it could fine tune with TRAINING_MAX_LENGTH = 1024 (previously OOM occurred at values over 720). Let's see this matchup to the hidden layer size improves the models produced  to yield better inferencing results.\n",
    "\n",
    "ep4lr1e-5ds2nfr12seed9973td.01P100len1k as an \"ensemble\" of one.... my mistake\n",
    "v35 - error, no GPU, also v36 and v37 will likely be redone with threshold 0.99\n",
    "v36 Ensemble test with checkpoint-4702 threshold 0.9, maxlen=2048, -> yielding LB=0.944\n",
    "v37 Ensemble test with checkpoint-9404 threshold 0.9, maxlen=2048, -> yielding LB=0.949\n",
    "v38 Ensemble test with checkpoint-4702 threshold 0.99, maxlen=2048, -> yielding LB=0.917\n",
    "v39 Ensemble test with checkpoint-9404 threshold 0.90, maxlen=2048, -> yielding LB=0.955\n",
    "v40 Ensemble test with checkpoint-14106 threshold 0.99, maxlen=2048, -> yielding LB=0.955\n",
    "v41 Ensemble test with checkpoint-18808 threshold 0.99, maxlen=2048, -> yielding LB=0.951\n",
    "\n",
    "This is super disturbing. Lower LB scores despite using longer strings of training tokens. Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v6) until it could fine tune with TRAINING_MAX_LENGTH = 512. Yes,this is an attempt to see if it helps to go to a smaller size. Assuming this does not help, I'll try to redesign training to chuckify the essays.\n",
    "\n",
    "v42 Ensemble test with checkpoint-4702 threshold 0.99, maxlen=2048 --> yielding LB=0.910\n",
    "v43 Ensemble test with checkpoint-9404 threshold 0.99, maxlen=2048 --> yielding LB=0.934\n",
    "Likely a no go situation, with just the first 512 tokens of esseys usedthe start of interencing is scoring low.\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v7) until it could fine tune with TRAINING_MAX_LENGTH = 512 chunks with all tokens included in training (but not smart chunking - as the last chunk gets a reminder without consideration of its size).\n",
    "\n",
    "v44 Ensemble test with ep1&9230&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.937\n",
    "v45 Ensemble test with ep2&18460&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.954\n",
    "v46 Ensemble test with ep3&27690&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.937\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v8) until it could fine tune with TRAINING_MAX_LENGTH = 720 chunks and lr=5e-6 with all tokens included in training (but not smart chunking - as the last chunk gets a reminder without consideration of its size).\n",
    "\n",
    "v47 Ensemble test with ep2&14504&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.955\n",
    "v48 Ensemble test with ep3&21756&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.954\n",
    "v49 Ensemble test with ep4&21756&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.955\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v9) until it could fine tune with TRAINING_MAX_LENGTH = 720 chunks and lr=1e-5 with all tokens included in training. Also going to try to double the batch size and allow 10% for validation to be more 'useful'. Best score of the past single model v11 of original training used a .1 split and showed worse training losses with a larger batch size. This is trn2 notebook version 12. 51Single-private-readyTrn2m12-c720P100tr.99ep1\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.002100\t0.001067\t0.979037\t0.965544\t0.978511\n",
    "2\t0.000600\t0.000585\t0.989130\t0.975996\t0.988619\n",
    "3\t0.000300\t0.000505\t0.991718\t0.987374\t0.991551\n",
    "4\t0.000100\t0.000519\t0.990942\t0.990173\t0.990912\n",
    "v50 model-v9 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.933\n",
    "v51 model-v9 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.961 *\n",
    "v52 model-v9 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.960\n",
    "v53 model-v9 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.944\n",
    "\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.002100\t0.001264\t0.974164\t0.969568\t0.973987\n",
    "2\t0.000800\t0.000901\t0.978905\t0.975437\t0.978771\n",
    "3\t0.000300\t0.000816\t0.981749\t0.986895\t0.981946\n",
    "4\t0.000200\t0.000898\t0.982934\t0.990683\t0.983230\n",
    "Trn2 version 13 ep4lr1e-5ds2nfr12seed421td.1P100chn720b4\n",
    "v54 model-v10 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.950\n",
    "v55 model-v10 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.953\n",
    "v56 model-v10 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.958 *\n",
    "v57 model-v10 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.956\n",
    "\n",
    "Trn2 version 14 ep4lr1e-5ds2nfr12seed8128td.1P100chn720b4\n",
    "V??Single-private-readyTrn2m14-c720P100tr.99ep?\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.001600\t0.000959\t0.978705\t0.965474\t0.978190\n",
    "2\t0.000400\t0.001077\t0.978916\t0.982646\t0.979059\n",
    "3\t0.000300\t0.000882\t0.982079\t0.991908\t0.982453\n",
    "4\t0.000100\t0.000940\t0.983344\t0.987090\t0.983487\n",
    "v60 model-v11 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.953\n",
    "v61 model-v11 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.963\n",
    "v62 model-v11 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.946\n",
    "v63 model-v11 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.958\n",
    "\n",
    "All Pairs\n",
    "v58 Ensemble test tv12&trn2-v9 with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v9\n",
    "v59 Ensemble test tv12&trn2-v10 with trshold 0.99, maxlen=2048, -> yielding LB=0.962\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v10\n",
    "v66 Ensemble test tv12&trn2-v11 with trshold 0.99, maxlen=2048, -> yielding LB=0.968\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v11\n",
    "\n",
    "All Triplets\n",
    "Top three modes by build setup: v64 Ensemble test tv12&best-trn2-v3&trn2-v11 LB=0.968\n",
    "A. 0.50 on V12 of /kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702 ep1\n",
    "B. 0.20 on Best-Models-Trn2-v3 at 0.962 => /kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v3-checkpoint-9404 ep1 Trn2 v12\n",
    "C. 0.30 V11 of /kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594 ep2 of Trn2 build v14\n",
    "\n",
    "Top three modes by build setup: v65 Ensemble test tv12&best-trn2-v3&trn2-v11 LB=0.968\n",
    "A. 0.34 on V12 of /kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702 ep1\n",
    "B. 0.33 on Best-Models-Trn2-v3 at 0.962 => /kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v3-checkpoint-9404 ep1 Trn2 v12\n",
    "C. 0.33 V11 of /kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594 ep2 of Trn2 build v14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4e3d1",
   "metadata": {
    "papermill": {
     "duration": 0.006076,
     "end_time": "2024-04-20T04:27:38.226470",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.220394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "My Training Notebook is at: https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-training\n",
    "\n",
    "This inferencing notebook's key ideas were originally built to work with...\n",
    "\n",
    "## ðŸ›‘ Wait a second - maybe also look at the training\n",
    "- My training notebook (containing equally many emojis) is here: I would love an upvote if you use the notebook or learned something new!\n",
    "- https://www.kaggle.com/code/valentinwerner/915-deberta3base-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5414dfe",
   "metadata": {
    "papermill": {
     "duration": 0.005934,
     "end_time": "2024-04-20T04:27:38.238546",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.232612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reserve 25% for models and system, then 25% for the public dataset.\n",
    "The other 50% is taken away for a test to minimize the risk of an OOM exception.\n",
    "Basically, with 75% of the data in the private set, it will be 3x the size of the public\n",
    "data set. A single model with the training set, with 16GB pulled out, is at 71% memory used which leaves just about the right amount that must suffice for the public test set.\n",
    "\n",
    "Reserving half the memory is probably overkill, given that the redesign to write the CSV file in batches of 50 documents and collect garbadge between the batches likely means the only linear growth will be in the initial Dataset object to feed the inferencing process (which remains done in one large batch of all json objects in the test set.) The rewrite seems to have little impact on wall time and so I will halt optimzation of the loops in favor of exploring inteferencing and ensemble based improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf44a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T17:42:29.009801Z",
     "iopub.status.busy": "2024-03-10T17:42:29.00942Z",
     "iopub.status.idle": "2024-03-10T17:42:48.308737Z",
     "shell.execute_reply": "2024-03-10T17:42:48.307696Z",
     "shell.execute_reply.started": "2024-03-10T17:42:29.009771Z"
    },
    "papermill": {
     "duration": 0.005812,
     "end_time": "2024-04-20T04:27:38.250752",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.244940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****# Comment out and make a Markup for contest submission\n",
    "#useToTestForPrivateSetSize = bytearray(int(20 * (1024 ** 3))) synatx error in case it is both uncommented and changed to a code block by accident\n",
    "\n",
    "## Initial parameters and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35dc12d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:27:38.265080Z",
     "iopub.status.busy": "2024-04-20T04:27:38.264737Z",
     "iopub.status.idle": "2024-04-20T04:27:38.275860Z",
     "shell.execute_reply": "2024-04-20T04:27:38.274990Z"
    },
    "papermill": {
     "duration": 0.020808,
     "end_time": "2024-04-20T04:27:38.277759",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.256951",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_MAX_LENGTH=2048 # 1024\n",
    "STRIDE=384\n",
    "\n",
    "# Note that training a model with stride, such as: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b\n",
    "# will also improve performance\n",
    "# the tokenizer will be taken from only the first model in a set of models\n",
    "# best choice is a model that differs only on the splits used.\n",
    "## model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-4702\"\n",
    "\n",
    "# Use for ensembles, and be sure to pin the needed version before running the code\n",
    "model_path = \"/kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702\" # pin v12 LB=0.965 at tr.99 maxlen2048\n",
    "\n",
    "# using the model path below to test a new trn2 bbuild with 1024 byte hidden/inference length (was 720)\n",
    "#model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-13188\"\n",
    "\n",
    "model_path2 = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594\"\n",
    "#model_path2 = \"/kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v2-checkpoint-14106\"\n",
    "model_path3 = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594\" # pin v11\n",
    "\n",
    "# Comment out when we have modified this for a single model run\n",
    "#model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-14106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cff9ae2",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-20T04:27:38.291359Z",
     "iopub.status.busy": "2024-04-20T04:27:38.290987Z",
     "iopub.status.idle": "2024-04-20T04:27:56.301666Z",
     "shell.execute_reply": "2024-04-20T04:27:56.300614Z"
    },
    "papermill": {
     "duration": 18.019805,
     "end_time": "2024-04-20T04:27:56.303901",
     "exception": false,
     "start_time": "2024-04-20T04:27:38.284096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset \n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Might revisit, but for now HF datasets module is not supported by the TPU / CUDA\n",
    "# I would need to rewrite without datasets to import Dataset\n",
    "# Comment out the import and device setting when using the GPU P100 or T4x2\n",
    "#import torch # we will use this to get some more GPU time from the TPUs\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# also, look for .to(device) and comment out those lines when not using the TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3b258",
   "metadata": {
    "papermill": {
     "duration": 0.006177,
     "end_time": "2024-04-20T04:27:56.316673",
     "exception": false,
     "start_time": "2024-04-20T04:27:56.310496",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## â™Ÿï¸ Data Loading & Data Tokenization\n",
    "- This tokenizer is actually special, comparing to usual NLP challenges\n",
    "- inference tokenizer is a bit different than training tokenizer, because we don't have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4baefa5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:27:56.331157Z",
     "iopub.status.busy": "2024-04-20T04:27:56.330514Z",
     "iopub.status.idle": "2024-04-20T04:27:56.337778Z",
     "shell.execute_reply": "2024-04-20T04:27:56.336862Z"
    },
    "papermill": {
     "duration": 0.016883,
     "end_time": "2024-04-20T04:27:56.339845",
     "exception": false,
     "start_time": "2024-04-20T04:27:56.322962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH, stride=STRIDE, return_overflowing_tokens=True)\n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27665a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:27:56.355119Z",
     "iopub.status.busy": "2024-04-20T04:27:56.354228Z",
     "iopub.status.idle": "2024-04-20T04:27:57.721706Z",
     "shell.execute_reply": "2024-04-20T04:27:57.720410Z"
    },
    "papermill": {
     "duration": 1.377761,
     "end_time": "2024-04-20T04:27:57.724329",
     "exception": false,
     "start_time": "2024-04-20T04:27:56.346568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d250618ab246b68a020dbe6c7d3358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32c035deca4404198fef77b2ec257ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [x[\"document\"] for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c96452d",
   "metadata": {
    "papermill": {
     "duration": 0.007111,
     "end_time": "2024-04-20T04:27:57.738753",
     "exception": false,
     "start_time": "2024-04-20T04:27:57.731642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ‹ðŸ»â€â™€ï¸ Trainer Class based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3f60c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:27:57.755460Z",
     "iopub.status.busy": "2024-04-20T04:27:57.754611Z",
     "iopub.status.idle": "2024-04-20T04:28:11.985883Z",
     "shell.execute_reply": "2024-04-20T04:28:11.985061Z"
    },
    "papermill": {
     "duration": 14.242328,
     "end_time": "2024-04-20T04:28:11.988262",
     "exception": false,
     "start_time": "2024-04-20T04:27:57.745934",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19259e02",
   "metadata": {
    "papermill": {
     "duration": 0.007969,
     "end_time": "2024-04-20T04:28:12.003610",
     "exception": false,
     "start_time": "2024-04-20T04:28:11.995641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This needs a second model build off the exact same LLM base (large in this case).\n",
    "It must be just another run as it must have the same tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c02bde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:28:12.019502Z",
     "iopub.status.busy": "2024-04-20T04:28:12.019124Z",
     "iopub.status.idle": "2024-04-20T04:28:25.339176Z",
     "shell.execute_reply": "2024-04-20T04:28:25.338389Z"
    },
    "papermill": {
     "duration": 13.330801,
     "end_time": "2024-04-20T04:28:25.341600",
     "exception": false,
     "start_time": "2024-04-20T04:28:12.010799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = AutoModelForTokenClassification.from_pretrained(model_path2)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator2 = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model=model2, \n",
    "    args=args, \n",
    "    data_collator=collator2, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "509824bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:28:25.357572Z",
     "iopub.status.busy": "2024-04-20T04:28:25.357172Z",
     "iopub.status.idle": "2024-04-20T04:28:26.629939Z",
     "shell.execute_reply": "2024-04-20T04:28:26.629060Z"
    },
    "papermill": {
     "duration": 1.283633,
     "end_time": "2024-04-20T04:28:26.632491",
     "exception": false,
     "start_time": "2024-04-20T04:28:25.348858",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3 = AutoModelForTokenClassification.from_pretrained(model_path3)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator3 = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3, \n",
    "    args=args, \n",
    "    data_collator=collator3, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d807f",
   "metadata": {
    "papermill": {
     "duration": 0.00737,
     "end_time": "2024-04-20T04:28:26.647462",
     "exception": false,
     "start_time": "2024-04-20T04:28:26.640092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Striding functions\n",
    "\n",
    "As using the stride give an overlap in tokens, these have to be removed (either pick one side of the stride or average them, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962c419c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:28:26.663289Z",
     "iopub.status.busy": "2024-04-20T04:28:26.662894Z",
     "iopub.status.idle": "2024-04-20T04:28:26.670494Z",
     "shell.execute_reply": "2024-04-20T04:28:26.669604Z"
    },
    "papermill": {
     "duration": 0.017779,
     "end_time": "2024-04-20T04:28:26.672458",
     "exception": false,
     "start_time": "2024-04-20T04:28:26.654679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backwards_map_preds(sub_predictions, max_len):\n",
    "    if max_len != 1: # nothing to map backwards if sequence is too short to be split in the first place\n",
    "        if i == 0:\n",
    "            # First sequence needs no SEP token (used to end a sequence)\n",
    "            sub_predictions = sub_predictions[:,:-1,:]\n",
    "        elif i == max_len-1:\n",
    "            # End sequence needs to CLS token + Stride tokens \n",
    "            sub_predictions = sub_predictions[:,1+STRIDE:,:] # CLS tokens + Stride tokens\n",
    "        else:\n",
    "            # Middle sequence needs to CLS token + Stride tokens + SEP token\n",
    "            sub_predictions = sub_predictions[:,1+STRIDE:-1,:]\n",
    "    return sub_predictions\n",
    "\n",
    "def backwards_map_(row_attribute, max_len):\n",
    "    # Same logics as for backwards_map_preds - except lists instead of 3darray\n",
    "    if max_len != 1:\n",
    "        if i == 0:\n",
    "            row_attribute = row_attribute[:-1]\n",
    "        elif i == max_len-1:\n",
    "            row_attribute = row_attribute[1+STRIDE:]\n",
    "        else:\n",
    "            row_attribute = row_attribute[1+STRIDE:-1]\n",
    "    return row_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f996287a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:28:26.688885Z",
     "iopub.status.busy": "2024-04-20T04:28:26.688528Z",
     "iopub.status.idle": "2024-04-20T04:28:32.216629Z",
     "shell.execute_reply": "2024-04-20T04:28:32.215572Z"
    },
    "papermill": {
     "duration": 5.538894,
     "end_time": "2024-04-20T04:28:32.218704",
     "exception": false,
     "start_time": "2024-04-20T04:28:26.679810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission.csv with pauses for GC to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,  0,7,9,B-NAME_STUDENT\n",
      "\n",
      "1 ,  1,7,10,I-NAME_STUDENT\n",
      "\n",
      "2 ,  2,7,482,B-NAME_STUDENT\n",
      "\n",
      "3 ,  3,7,483,I-NAME_STUDENT\n",
      "\n",
      "4 ,  4,7,741,B-NAME_STUDENT\n",
      "\n",
      "5 ,  5,7,742,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 ,  6,10,0,B-NAME_STUDENT\n",
      "\n",
      "7 ,  7,10,1,I-NAME_STUDENT\n",
      "\n",
      "8 ,  8,10,464,B-NAME_STUDENT\n",
      "\n",
      "9 ,  9,10,465,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 ,  10,16,4,B-NAME_STUDENT\n",
      "\n",
      "11 ,  11,16,5,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 ,  12,20,5,B-NAME_STUDENT\n",
      "\n",
      "13 ,  13,20,6,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ,  14,56,12,B-NAME_STUDENT\n",
      "\n",
      "15 ,  15,56,13,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ,  16,86,6,B-NAME_STUDENT\n",
      "\n",
      "17 ,  17,86,7,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 ,  18,93,0,B-NAME_STUDENT\n",
      "\n",
      "19 ,  19,93,1,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ,  20,104,7,B-NAME_STUDENT\n",
      "\n",
      "21 ,  21,104,8,B-NAME_STUDENT\n",
      "\n",
      "22 ,  22,104,9,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ,  23,112,5,B-NAME_STUDENT\n",
      "\n",
      "24 ,  24,112,6,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ,  25,123,32,B-NAME_STUDENT\n",
      "\n",
      "26 ,  26,123,33,I-NAME_STUDENT\n",
      "\n",
      "27 ,  27,123,38,B-NAME_STUDENT\n",
      "\n",
      "CPU times: user 5.08 s, sys: 138 ms, total: 5.22 s\n",
      "Wall time: 5.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "gcnow = 0\n",
    "giverowid = 0\n",
    "ansrow = 0\n",
    "\n",
    "print(\"Writing submission.csv with pauses for GC to work.\")\n",
    "with open(\"submission.csv\", 'w') as csv_file:\n",
    "    submission = pd.DataFrame(columns= [\"row_id\", \"document\", \"token\", \"label\"])\n",
    "    submission.to_csv(csv_file, header=True, index=False)\n",
    "\n",
    "    preds = []\n",
    "    \n",
    "    for row in ds:\n",
    "        if gcnow % 50 == 49:\n",
    "            _ = gc.collect()\n",
    "        gcnow += 1\n",
    "        \n",
    "        # keys that need to be re-assembled\n",
    "        row_preds = []\n",
    "        row_offset = []\n",
    "        ds_dict = {\n",
    "            \"document\":[],\n",
    "            \"token_map\":[],\n",
    "            \"offset_mapping\":[],\n",
    "            \"tokens\":[]\n",
    "        }\n",
    "        \n",
    "        # Build a dictionary for reassembling the predictions and call for the inference\n",
    "        for i, y in enumerate(row[\"offset_mapping\"]):\n",
    "            # create new datasset for each of of the splits per document\n",
    "            x = Dataset.from_dict({\n",
    "                \"token_type_ids\":[row[\"token_type_ids\"][i]],\n",
    "                \"input_ids\":[row[\"input_ids\"][i]],\n",
    "                \"attention_mask\":[row[\"attention_mask\"][i]],\n",
    "                \"offset_mapping\":[row[\"offset_mapping\"][i]]\n",
    "            })\n",
    "            # predict for that split\n",
    "            #pred = trainer.predict(x).predictions\n",
    "            #pred = (trainer.predict(x).predictions + trainer2.predict(x).predictions)/2.0\n",
    "            pred = 0.9 * trainer.predict(x).predictions + 0.1 * trainer2.predict(x).predictions\n",
    "            #pred = 0.34 * trainer.predict(x).predictions + 0.33 * trainer2.predict(x).predictions + 0.33 * trainer3.predict(x).predictions\n",
    "            \n",
    "            # removing the stride and additional CLS & SEP that are created\n",
    "            row_preds.append(backwards_map_preds(pred, len(row[\"offset_mapping\"])))\n",
    "            row_offset += backwards_map_(y, len(row[\"offset_mapping\"]))\n",
    "   \n",
    "        # Complete the prediction\n",
    "        preds_final = []\n",
    "        p_concat = np.concatenate(row_preds, axis = 1)\n",
    "        predictions_softmax = np.exp(p_concat) / np.sum(np.exp(p_concat), axis = 2).reshape(p_concat.shape[0],p_concat.shape[1],1)\n",
    "        p_concat = p_concat.argmax(-1) # the highest probability class of PII predicted\n",
    "        predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n",
    "        O_predictions = predictions_softmax[:,:,12] # the probability that it is not PII (aka class \"O\")\n",
    "        # this threshold seems more of an art than a science unless I can find a function to estimate or optimize it\n",
    "        threshold = 0.9412 # 0.97 # 0.99 # the threshold of confidence below which we assume the prediction is a class of PII\n",
    "        preds_final.append(np.where(O_predictions < threshold, predictions_without_O , p_concat))\n",
    "\n",
    "        ds_dict[\"document\"].append(row[\"document\"])\n",
    "        ds_dict[\"tokens\"].append(row[\"tokens\"])\n",
    "        ds_dict[\"token_map\"].append(row[\"token_map\"])\n",
    "        ds_dict[\"offset_mapping\"].append(row_offset)\n",
    "        ds = Dataset.from_dict(ds_dict)\n",
    "        pairs = []\n",
    "        document, token, label, token_str = [], [], [], []\n",
    "\n",
    "        for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n",
    "            for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n",
    "                label_pred = id2label[str(token_pred)]\n",
    "\n",
    "                if start_idx + end_idx == 0: continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map): break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "\n",
    "                # ignore \"O\" predictions and whitespace preds\n",
    "                if label_pred != \"O\" and token_id != -1:\n",
    "                    pair=(doc, token_id)\n",
    "                    if pair not in pairs:\n",
    "                        pairs.append(pair) # we will need to remember the pairing\n",
    "                        rowstr = str(giverowid)+\",\"+str(doc)+\",\"+str(token_id)+\",\"+label_pred+\"\\n\"\n",
    "                        print(ansrow, \", \", rowstr)\n",
    "                        ansrow += 1\n",
    "                        csv_file.write(rowstr)\n",
    "                        giverowid += 1\n",
    "        \n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37471709",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-20T04:28:32.243290Z",
     "iopub.status.busy": "2024-04-20T04:28:32.242892Z",
     "iopub.status.idle": "2024-04-20T04:28:32.250584Z",
     "shell.execute_reply": "2024-04-20T04:28:32.249560Z"
    },
    "papermill": {
     "duration": 0.022339,
     "end_time": "2024-04-20T04:28:32.252653",
     "exception": false,
     "start_time": "2024-04-20T04:28:32.230314",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.36 GB\n",
      "Available RAM: 27.29 GB\n",
      "Used RAM: 3.61 GB\n",
      "Memory Usage: 13.0%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the memory details\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "total_memory = memory.total / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Total RAM: {total_memory:.2f} GB\")\n",
    "\n",
    "available_memory = memory.available / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Available RAM: {available_memory:.2f} GB\")\n",
    "\n",
    "used_memory = memory.used / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Used RAM: {used_memory:.2f} GB\")\n",
    "\n",
    "memory_usage_percent = memory.percent\n",
    "print(f\"Memory Usage: {memory_usage_percent}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4466098,
     "isSourceIdPinned": true,
     "sourceId": 7767497,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4601755,
     "sourceId": 7847820,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4568299,
     "isSourceIdPinned": true,
     "sourceId": 8013872,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 60.572048,
   "end_time": "2024-04-20T04:28:35.315589",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-20T04:27:34.743541",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "068b347a9b874010990d30e29d58ab69": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "077f6358026f486384924be085588a37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19201f8c72ea4e49a45fb8858cb89ec8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_c4fe93910464495f8e1fd98e5fdc6db2",
       "value": " 5/5 [00:00&lt;00:00, 66.35ex/s]"
      }
     },
     "19201f8c72ea4e49a45fb8858cb89ec8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "318639b9087b40adb812527256354753": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3d6b23f00d834745b0ef4a6ac06c7279": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6c1448b94fb54a5eb8efecfab25b66cb",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_75cbafb4cb0e4f09a746642aa4f60008",
       "value": 5.0
      }
     },
     "6c1448b94fb54a5eb8efecfab25b66cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "75cbafb4cb0e4f09a746642aa4f60008": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "79b4aaf60ec8436fb99dfaffd0d724d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7a4a2f8439264b91a802294460dd2bc2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7cb7fc69d08d4002929b04701b0ef1bc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8dafeec55536425d97d5fc78b75f60af",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ecaeb5e5bf20437a8f72df0922ba2820",
       "value": "#1: 100%"
      }
     },
     "8dafeec55536425d97d5fc78b75f60af": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "950e81ea2dcf4b82b1a1a08f7707f4ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9875ed4cccf54b83ba942fa38472d52e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7a4a2f8439264b91a802294460dd2bc2",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_950e81ea2dcf4b82b1a1a08f7707f4ec",
       "value": 5.0
      }
     },
     "b80dc192b0a6474990743a405c3a65a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_068b347a9b874010990d30e29d58ab69",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_ff2f63d9e1be4cdea5622cf8645fcbad",
       "value": " 5/5 [00:00&lt;00:00, 55.57ex/s]"
      }
     },
     "bde3dcdef7d04403bc52b2007ca6bf55": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c32c035deca4404198fef77b2ec257ce": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7cb7fc69d08d4002929b04701b0ef1bc",
        "IPY_MODEL_3d6b23f00d834745b0ef4a6ac06c7279",
        "IPY_MODEL_077f6358026f486384924be085588a37"
       ],
       "layout": "IPY_MODEL_318639b9087b40adb812527256354753"
      }
     },
     "c4fe93910464495f8e1fd98e5fdc6db2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ce9398e60d2247de8b8d499f8e3790a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_bde3dcdef7d04403bc52b2007ca6bf55",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_79b4aaf60ec8436fb99dfaffd0d724d2",
       "value": "#0: 100%"
      }
     },
     "e06784367e9043138d82d5f370813f24": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e2d250618ab246b68a020dbe6c7d3358": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_ce9398e60d2247de8b8d499f8e3790a0",
        "IPY_MODEL_9875ed4cccf54b83ba942fa38472d52e",
        "IPY_MODEL_b80dc192b0a6474990743a405c3a65a6"
       ],
       "layout": "IPY_MODEL_e06784367e9043138d82d5f370813f24"
      }
     },
     "ecaeb5e5bf20437a8f72df0922ba2820": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "ff2f63d9e1be4cdea5622cf8645fcbad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
