{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b91815c",
   "metadata": {
    "papermill": {
     "duration": 0.006585,
     "end_time": "2024-04-08T01:35:01.290166",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.283581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸŸï¸ Credits\n",
    "\n",
    "- Stride is something Raja first shared: https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/473011\n",
    "- The biggest booster in performance is changing triplets to pairs in token reassembling: https://www.kaggle.com/code/nbroad/transformer-ner-baseline-lb-0-854/comments#2659393\n",
    "\n",
    "Updates and improvements made by Thomas Gamet\n",
    "This notebook is under the MIT License.\n",
    "Will test out of striding on inference alone helps my deberta-v3-large trained model\n",
    "This came from: https://www.kaggle.com/code/valentinwerner/945-deberta-3-base-striding-inference\n",
    "\n",
    "This particular notebook was forked from a prior version of mine when I realized that it was using more than 1/3rd the memory available to complete a leader board submission. This notebook was rewritten to write its submission file by immediately reassembling inferences and writing them to the submission csv file. The first memory efficiency rewrite seems to have heavy dictionary and Dataset initialization, but I'm not sure (yet) if it makes a significant performance difference (just a note if time limits become an issue)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21689583",
   "metadata": {
    "papermill": {
     "duration": 0.005761,
     "end_time": "2024-04-08T01:35:01.302071",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.296310",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Restructured to write predictions immediately to the submission file\n",
    "\n",
    "With the private dataset expected to be 3 times larger than the public dataset I suspect I need to get the code to run in a third of the free space memory structure. It now seems I need to avoid building up all predictions and the ds_dict as 25% of available RAM is insufficient to maintain all of these structures. It's going to be a bit \"ugly\" but not really complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9ea4d4",
   "metadata": {
    "papermill": {
     "duration": 0.005714,
     "end_time": "2024-04-08T01:35:01.313751",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.308037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "With the threshold at 0.99.\n",
    "The deberta-v3-large using 12 of 24 frozen layers, lr=1e-5, 2 data sets, and 2 epochs it scored an LB=0.956\n",
    "The deberta-v3-large using 6 of 24 frozen layers, lr=5e-6, 2 data sets, and 4 epochs it scored an LB=0.952\n",
    "\n",
    "Multiple tests were run with ep2lr1e-5ds2fr12seed421+.01testsplit\n",
    "v4 threshold 0.99 LB=0.941\n",
    "v5 threshold 0.97 LB=0.947\n",
    "v6 threshold 0.90 LB=0.947\n",
    "v7 threshold 0.98 LB=0.946\n",
    "v8 threshold 0.94 LB=0.948\n",
    "It seems definitive two epochs with 99% of the training data is not producing good results. Best bet is to try several thresholds with 1 and 3 epochs of training aiming for the LB scores to be like a cross validation score. If that does not work well enough then the full training may require smaller learning rates to do well, or more frozen layers, which is to be determined.\n",
    "\n",
    "Loading Dataset v6 We now have 3 epochs with ep2lr1e-5ds2fr12seed421+.01td\n",
    "/kaggle/input/piidd-extending-large-model-starter-model/output/\n",
    "v9 threshold 0.99 LB=0.920 (checkpoint-4535)\n",
    "v10 threshold 0.99 LB=0.958 (checkpoint-13605)\n",
    "v11 threshold 0.97 LB=0.949 (checkpoint-13605)\n",
    "v12 threshold dynamic LB=0.953 (checkpoint-13605) - highest PII confidence -0.0075\n",
    "v13 threshold dynamic LB=0.952 (checkpoint-13605) - highest PII confidence -0.0100\n",
    "New strategy => pick the model epoch that gives the best LB for a threshold of .99\n",
    "\n",
    "Loaded Dataet v7 /kaggle/input/piidd-extending-large-model-starter-\n",
    "ep4lr1e-5ds3fr12seed421+.01td epochs 4\n",
    "model/output/checkpoint-26920\n",
    "v14 threshold 0.99 LB=0.932\n",
    "model/output/checkpoint-20190\n",
    "v15 error running without GPU, need to wait for it to faill\n",
    "v16 threshold 0.99 LB=0.940\n",
    "model/output/checkpoint-13460\n",
    "v17 threshold 0.99 LB=0.951\n",
    "model/output/checkpoint-6730\n",
    "v18 threshold 0.99 LB=0.930\n",
    "\n",
    "ep#lr2e-6ds3fr12seed42+.1td-of5ep\n",
    "Loaded Dataet v8 /kaggle/input/piidd-extending-large-model-starter-\n",
    "model/output/checkpoint-6118\n",
    "v19 threshold 0.99 LB=0.924\n",
    "model/output/checkpoint-12236\n",
    "v20 threshold 0.99 LB=0.945\n",
    "model/output/checkpoint-18354\n",
    "v21 threshold 0.99 LB=0.933\n",
    "model/output/checkpoint-24472\n",
    "v22 threshold 0.99 LB=0.935\n",
    "model/output/checkpoint-30590\n",
    "v23 was not a submission\n",
    "v24 threshold 0.99 LB=0.934\n",
    "\n",
    "ep#lr2e-5ds3fr12seed42+.1td+of3ep\n",
    "Loaded Dataset v9 /kaggle/input/piidd-extending-large-model-starter-\n",
    "model/output/checkpoint-6118\n",
    "v25 threshold 0.99 LB=0.942\n",
    "model/output/checkpoint-12236\n",
    "v26 threshold 0.99 LB=0.948\n",
    "model/output/checkpoint-18354\n",
    "v27 threshold 0.99 LB=0.945\n",
    "\n",
    "Loaded Dataset v10 \n",
    "ep#lr5e-6ds4fr12seed42td.1tb4vb8\n",
    "checkpoint-10527 which is ep3\n",
    "v28 threshold 0.99 LB=0.939\n",
    "checkpoint-14036 which is ep4\n",
    "v29 threshold 0.99 LB=0.942\n",
    "\n",
    "Trained v11 v11 ep4lr9e-6ds4fr18seed42td.1tb4vb8\n",
    "checkpoint-14036 which is ep4 with the highest training F1 score\n",
    "v30 threshold 0.99 LB=0.938\n",
    "\n",
    "Also, an experiment or two with 2048 and 720 for INFERENCE_MAX_LENGTH is needed.\n",
    "v34 using training v11 ep3lr1e-5ds2nfr12seed421td.1 threshold 0.99\n",
    "LB at maxlen=2048 was 0.953\n",
    "LB at maxlen=720 was 0.951\n",
    "LB at maxlen=1024 was 0.952\n",
    "\n",
    "v35 using training v12 ep5lr1e-5ds2nfr12seed421td.01 threashold 0.99\n",
    "LB at maxlen=1024 with epoch 4 of 5 was 0.947\n",
    "v36 LB at maxlen=1024 with epoch 3 of 5 was 0.948\n",
    "v37 LB at maxlen=1024 with epoch 5 of 5 was 0.949\n",
    "v38 LB at maxlen=1024 with epoch 2 of 5 was 0.958\n",
    "v39 LB at maxlen=1024 with epoch 1 of 5 was 0.963 (short lived winner)\n",
    "v40 LB at maxlen=2048 with epoch 1 of 5 was 0.965 (new winner)\n",
    "\n",
    "The aim here is a similar LB with 3 different training datasets to see if they can form an ensemble. Theory, training with all datasets together decreases generalization. They high learning rate, compared to what the model specs say it was trained at, are likely the reason why it both 'learns' in an epoch or two, and why it soon starts loosing performance (overfits on its training data and loses generalization).\n",
    "Training v22 ep2lr1e-5ds3nfr12seed421td.01\n",
    "Concern, I have to basically negate the threshold to 0.001 to get it to filter down answers.\n",
    "v41 LB at maxlen=2048 with epoch 1 of 2 was 0.906\n",
    "v42 LB at maxlen=2048 with epoch 2 of 2 was 0.915\n",
    "This dataset is not promissing.\n",
    "\n",
    "Trying with v14 in the model dataset v23ep3lr1e-5ds2nfr12seed421td.01\n",
    "v43 LB at maxlen-2048 with epoch 1 of 3 was 0.934\n",
    "v44 LB at maxlen-2048 with epoch 2 of 3 was 0.948\n",
    "v45 LB at maxlen-2048 with epoch 3 of 3 was 0.954\n",
    "We need more epochs to see if at which one it reaches the point of an overfit.\n",
    "v46 LB at maxlen-2048 with epoch 4 of 6 was 0.941 which shows we have begun overfitting\n",
    "\n",
    "Next move, rebuild v12 with slight alterations to see if it helps in an ensemble.\n",
    "\n",
    "v46 ran with an error - wrong model checkpoint selection\n",
    "v47 confirming we rerun v40 with Trained model v12 /kaggle/input/piidd-extending-large-model-starter-model\n",
    "v48 seeing if v1 of trn2-model is like v12 above (two more epochs to try) -> LB=0.94\n",
    "That is different, but there are two more epochs to test as if an LB was a CV.\n",
    "v49 with v1 epoch 2 -> LB=0.958\n",
    "v50 with v1 epoch 3 -> LB=? (mislabeled run, it really is epoch 3's model output). \n",
    "\n",
    "**** New Direct CSV generation on a per document's prediction.\n",
    "\n",
    "v1 LB=0.963 recreating Training v12 ep5lr1e-5ds2nfr12seed421td.01 threashold 0.99 results but with 16GB reserved for the private dataset size expansion on a max length 1024 tokens\n",
    "v2 LB-0.965 provided all of V1 works but with 2048 allowed. It is also good to note that I believe the memory footprint is now much lower so if this works OOM should no longer be a major concern.\n",
    "v3 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048 for the competition at an LB=0.965 with full RAM available.\n",
    "v4 builds on trn2 v02-ep1of4lr1e-5ds2nfr12seed9973tr.01, threshold 0.99, maxlen-2048 for competition at an LB=0.952 with full RAM and epoch 0.\n",
    "v5 builds on trn2 trn2v2-ep2of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.945 with full RAM and epoch 1.\n",
    "v6 builds on trn2 trn2v2-ep3of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.959 with full RAM and epoch 2.\n",
    "v7 builds on trn2 trn2v2-ep4of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048 for competition at an LB=0.947 with full RAM and epoch 2.\n",
    "\n",
    "v8 Ensemble test yielding LB=0.967 new high\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v2-ep3of4lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "Also, this run will remove 20GB of RAM to confirm more than enough memory exists.\n",
    "v9 is the same ensemble with all memory available.\n",
    "\n",
    "v10 Ensemble test yielding LB=0.968 new high\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v1-ep3of3lr1e-5ds2nfr12seed4221td.01, threshold 0.99, maxlen-2048\n",
    "  \n",
    "Single model run with trn2v3-ep4lr1e-5ds2nfr12seed8128td.01\n",
    "v11 with Epoch 1 produces LB=0.910\n",
    "v12 with Epoch 2 produces LB=0.962\n",
    "v13 with Epoch 3 produces LB=0.952\n",
    "\n",
    "v14 Ensemble test yielding LB=0.967\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v3-ep2of3lr1e-5ds2nfr12seed8128td.01, threshold 0.99, maxlen=2048\n",
    "  \n",
    "v15 Ensemble test for memory footprint yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. trn2 trn2v2-ep3of3lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "  c. trn2 trn2v2-ep1of3lr1e-5ds2nfr12seed9973td.01, threshold 0.99, maxlen-2048\n",
    "It finished in 2.75 hours with 18GB of memory reserved for the private set size.\n",
    "The LB=0.965 (which is fine as the individual models had 0.965, 0.959, 0.952).\n",
    "Though this will be close with the private data size being 3x the public data size I will try at least one three model run maximizing the LB. The other two selections will use good, but not optimal, LBs based upon multiple prior experiences where the mid to upper mid CV's tend to produce the best LB scores, and I'm pretty much treating the LB like a CV score.\n",
    "\n",
    "The [Best-Trn2] contains:\n",
    "v1 is ep3lr1e-5ds2nfr12seed4221td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v1] epoch 3 LB=0.961\n",
    "v2 is ep4lr1e-5ds2nfr12seed9973td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v2] epoch 3 LB=0.959\n",
    "v3 is ep4lr1e-5ds2nfr12seed8128td.01 [https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v3] epoch 2 LB=0.962 which is Trn2 v3\n",
    "\n",
    "v16 was an invalid build attempt with the original model not correctly pinned.\n",
    "\n",
    "v17 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v1\n",
    "  c. Model-trn2-v2\n",
    "  \n",
    "v18 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v1\n",
    "  c. Model-trn2-v3\n",
    "  \n",
    "v19 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. Model-trn2-v2\n",
    "  c. Model-trn2-v3\n",
    "  \n",
    "v20 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.961\n",
    "  a. Model-trn2-v1\n",
    "  b. Model-trn2-v2\n",
    "  c. Model-trn2-v3\n",
    "\n",
    "Well, this was disappointing, a three model ensemble failed to produce an improved score, but with that said, nont all models have equal weight. A few more tries with weights on v18 which combines the top three individual models.\n",
    "\n",
    "v21 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.965\n",
    "  a. 0.4 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.3 Model-trn2-v1\n",
    "  c. 0.3 Model-trn2-v3\n",
    "  \n",
    "v22 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.25 Model-trn2-v1\n",
    "  c. 0.25 Model-trn2-v3\n",
    "  \n",
    "v23 Ensemble test with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=2048\n",
    "  b. 0.2 Model-trn2-v1\n",
    "  c. 0.3 Model-trn2-v3\n",
    "  \n",
    "v24 was v23 with maxlen on inference set to 4096 and testing for OOM. It ran to completion, and on time, but scored only 0.969.\n",
    "\n",
    "v25 Ensemble test with trshold 0.99, maxlen=4096, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01, threashold 0.99, maxlen=4096\n",
    "  b. 0.25 Model-trn2-v1\n",
    "  c. 0.25 Model-trn2-v3\n",
    "  \n",
    "Left weights at .5 .25 .25\n",
    "v26 Ensemble test with trshold 0.970, maxlen=2048, -> yielding LB=0.967\n",
    "v27 Ensemble test with trshold 0.995, maxlen=2048, -> yielding LB=0.967\n",
    "\n",
    "v28 Ensemble test with trshold 0.970, maxlen=2048, w.34.33.33 -> yielding LB=0.967\n",
    "v29 Ensemble test with trshold 0.995, maxlen=2048, w.34.33.33 -> yielding LB=0.967\n",
    "\n",
    "OK, it seems we are stuck and for the LB a threshold of 0.99 has proven the most effective.\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v4) until it could fine tune with TRAINING_MAX_LENGTH = 1024 (previously OOM occurred at values over 720). Let's see this matchup to the hidden layer size improves the models produced  to yield better inferencing results.\n",
    "\n",
    "ep4lr1e-5ds2nfr12seed8128td.01GPUP100tml1k as an \"ensemble\" of one.... my mistake\n",
    "v30 Ensemble test with checkpoint-4702 thrshold 0.99, maxlen=2048, -> yielding LB=0.923\n",
    "v31 Ensemble test with checkpoint-9404 thrshold 0.99, maxlen=2048, -> yielding LB=0.956\n",
    "v32 Ensemble test with checkpoint-14106 thrshold 0.99, maxlen=2048, -> yielding LB=0.955\n",
    "v33 Ensemble test with checkpoint-18808 thrshold 0.99, maxlen=2048, -> yielding LB=0.954\n",
    "\n",
    "v34 Ensemble test with checkpoint-9404 threshold 0.90, maxlen=2048 -> yielding LB=0.948\n",
    "Reducing the threshold increases the nummber of tokens seen as class \"O\" (not PII) which means positive PII detection decreases, and this lowers the score even though recall is weights over precision. \n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v5) until it could fine tune with TRAINING_MAX_LENGTH = 1024 (previously OOM occurred at values over 720). Let's see this matchup to the hidden layer size improves the models produced  to yield better inferencing results.\n",
    "\n",
    "ep4lr1e-5ds2nfr12seed9973td.01P100len1k as an \"ensemble\" of one.... my mistake\n",
    "v35 - error, no GPU, also v36 and v37 will likely be redone with threshold 0.99\n",
    "v36 Ensemble test with checkpoint-4702 threshold 0.9, maxlen=2048, -> yielding LB=0.944\n",
    "v37 Ensemble test with checkpoint-9404 threshold 0.9, maxlen=2048, -> yielding LB=0.949\n",
    "v38 Ensemble test with checkpoint-4702 threshold 0.99, maxlen=2048, -> yielding LB=0.917\n",
    "v39 Ensemble test with checkpoint-9404 threshold 0.90, maxlen=2048, -> yielding LB=0.955\n",
    "v40 Ensemble test with checkpoint-14106 threshold 0.99, maxlen=2048, -> yielding LB=0.955\n",
    "v41 Ensemble test with checkpoint-18808 threshold 0.99, maxlen=2048, -> yielding LB=0.951\n",
    "\n",
    "This is super disturbing. Lower LB scores despite using longer strings of training tokens. Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v6) until it could fine tune with TRAINING_MAX_LENGTH = 512. Yes,this is an attempt to see if it helps to go to a smaller size. Assuming this does not help, I'll try to redesign training to chuckify the essays.\n",
    "\n",
    "v42 Ensemble test with checkpoint-4702 threshold 0.99, maxlen=2048 --> yielding LB=0.910\n",
    "v43 Ensemble test with checkpoint-9404 threshold 0.99, maxlen=2048 --> yielding LB=0.934\n",
    "Likely a no go situation, with just the first 512 tokens of esseys usedthe start of interencing is scoring low.\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v7) until it could fine tune with TRAINING_MAX_LENGTH = 512 chunks with all tokens included in training (but not smart chunking - as the last chunk gets a reminder without consideration of its size).\n",
    "\n",
    "v44 Ensemble test with ep1&9230&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.937\n",
    "v45 Ensemble test with ep2&18460&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.954\n",
    "v46 Ensemble test with ep3&27690&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.937\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v8) until it could fine tune with TRAINING_MAX_LENGTH = 720 chunks and lr=5e-6 with all tokens included in training (but not smart chunking - as the last chunk gets a reminder without consideration of its size).\n",
    "\n",
    "v47 Ensemble test with ep2&14504&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.955\n",
    "v48 Ensemble test with ep3&21756&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.954\n",
    "v49 Ensemble test with ep4&21756&chn512 threshold 0.99, maxlen=2048 --> yielding LB=0.955\n",
    "\n",
    "Tweeked Trn2 (https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-trn2 v9) until it could fine tune with TRAINING_MAX_LENGTH = 720 chunks and lr=1e-5 with all tokens included in training. Also going to try to double the batch size and allow 10% for validation to be more 'useful'. Best score of the past single model v11 of original training used a .1 split and showed worse training losses with a larger batch size. This is trn2 notebook version 12. 51Single-private-readyTrn2m12-c720P100tr.99ep1\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.002100\t0.001067\t0.979037\t0.965544\t0.978511\n",
    "2\t0.000600\t0.000585\t0.989130\t0.975996\t0.988619\n",
    "3\t0.000300\t0.000505\t0.991718\t0.987374\t0.991551\n",
    "4\t0.000100\t0.000519\t0.990942\t0.990173\t0.990912\n",
    "v50 model-v9 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.933\n",
    "v51 model-v9 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.961 *\n",
    "v52 model-v9 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.960\n",
    "v53 model-v9 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.944\n",
    "\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.002100\t0.001264\t0.974164\t0.969568\t0.973987\n",
    "2\t0.000800\t0.000901\t0.978905\t0.975437\t0.978771\n",
    "3\t0.000300\t0.000816\t0.981749\t0.986895\t0.981946\n",
    "4\t0.000200\t0.000898\t0.982934\t0.990683\t0.983230\n",
    "Trn2 version 13 ep4lr1e-5ds2nfr12seed421td.1P100chn720b4\n",
    "v54 model-v10 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.950\n",
    "v55 model-v10 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.953\n",
    "v56 model-v10 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.958 *\n",
    "v57 model-v10 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.956\n",
    "\n",
    "Trn2 version 14 ep4lr1e-5ds2nfr12seed8128td.1P100chn720b4\n",
    "V??Single-private-readyTrn2m14-c720P100tr.99ep?\n",
    "Epoch\tTraining Loss\tValidation Loss\tRecall\tPrecision\tF1\n",
    "1\t0.001600\t0.000959\t0.978705\t0.965474\t0.978190\n",
    "2\t0.000400\t0.001077\t0.978916\t0.982646\t0.979059\n",
    "3\t0.000300\t0.000882\t0.982079\t0.991908\t0.982453\n",
    "4\t0.000100\t0.000940\t0.983344\t0.987090\t0.983487\n",
    "v60 model-v11 test with ep1 threshold 0.99, maxlen=2048 --> yielding LB=0.953\n",
    "v61 model-v11 test with ep2 threshold 0.99, maxlen=2048 --> yielding LB=0.963 *\n",
    "v62 model-v11 test with ep3 threshold 0.99, maxlen=2048 --> yielding LB=0.946\n",
    "v63 model-v11 test with ep4 threshold 0.99, maxlen=2048 --> yielding LB=0.958\n",
    "\n",
    "All Pairs\n",
    "v58 Ensemble test tv12&trn2-v9 with trshold 0.99, maxlen=2048, -> yielding LB=0.969\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v9\n",
    "v59 Ensemble test tv12&trn2-v10 with trshold 0.99, maxlen=2048, -> yielding LB=0.962\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v10\n",
    "v66 Ensemble test tv12&trn2-v11 with trshold 0.99, maxlen=2048, -> yielding LB=0.968\n",
    "  a. 0.5 builds train v12 ep5lr1e-5ds2nfr12seed421td.01\n",
    "  b. 0.5 Model-trn2-v11\n",
    "\n",
    "All Triplets\n",
    "Top three modes by build setup: v64 Ensemble test tv12&best-trn2-v3&trn2-v11 LB=0.968\n",
    "A. 0.50 on V12 of /kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702 ep1\n",
    "B. 0.20 on Best-Models-Trn2-v3 at 0.962 => /kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v3-checkpoint-9404 ep1 Trn2 v12\n",
    "C. 0.30 V11 of /kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594 ep2 of Trn2 build v14\n",
    "\n",
    "Top three modes by build setup: v65 Ensemble test tv12&best-trn2-v3&trn2-v11 LB=0.968\n",
    "A. 0.34 on V12 of /kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702 ep1\n",
    "B. 0.33 on Best-Models-Trn2-v3 at 0.962 => /kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v3-checkpoint-9404 ep1 Trn2 v12\n",
    "C. 0.33 V11 of /kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594 ep2 of Trn2 build v14\n",
    "\n",
    "Top three modes by build setup: v65 Ensemble test tv12&best-trn2-v3&trn2-v11 LB=0.968\n",
    "A. 0.4 on V12 of /kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702 ep1\n",
    "B. 0.2 on Best-Models-Trn2-v3 at 0.962 => /kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v3-checkpoint-9404 ep1 Trn2 v12\n",
    "C. 0.4 V11 of /kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-6594 ep2 of Trn2 build v14\n",
    "\n",
    "Reality Check, the private leader board has 75% of the data, which means it will take 3 times longer to complete inferencing than the public leader board data set. Bottom line, only the ensembles of 2 models will (or are expected) to complete on time.\n",
    "\n",
    "Let's run all pairs for the Best2 of Trn2 which are the best individual scoring models of Trn2 which used 720 training length, all data in chunks: ep5lr1e-5ds2nfr12seed####td.1\n",
    "\n",
    "Pair v68 best2-trn2-v9,best2-trn2-v10 has LB=0.964\n",
    "Pair v69 best2-trn2-v9,best2-trn2-v11 has LB=0.961\n",
    "Pair v70 best2-trn2-v10,best2-trn2-v11 has LB=0.967\n",
    "\n",
    "Test to see how well a single xsmall deberta model tuned with our data can perform.\n",
    "v71 uses model-trn2-v12 (trn2-v21) threshold=0.51 epoch 4 to get LB=0.914\n",
    "sample_submission had 25 true positives 0 false negatives and 2 false positives\n",
    "v72 uses model-trn2-v12 (trn2-v21) threshold=0.96 epoch 4 to get LB=0.942\n",
    "sample_submission had 25 true positives 0 false negatives and 8 false positives\n",
    "v73 uses model-trn2-v12 (trn2-v21) threshold=0.86 epoch 4 to get LB=0.\n",
    "sample_submission had 25 true positives 0 false negatives and 5 false positives\n",
    "v74 uses model-trn2-v12 (trn2-v21) threshold=0. epoch 4 to get LB=0.\n",
    "sample_submission had 25 true positives 0 false negatives and 11 false positives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459ea52",
   "metadata": {
    "papermill": {
     "duration": 0.005666,
     "end_time": "2024-04-08T01:35:01.325337",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.319671",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "My Training Notebook is at: https://www.kaggle.com/code/thomasgamet/piidd-extending-large-model-starter-training\n",
    "\n",
    "This inferencing notebook's key ideas were originally built to work with...\n",
    "\n",
    "## ðŸ›‘ Wait a second - maybe also look at the training\n",
    "- My training notebook (containing equally many emojis) is here: I would love an upvote if you use the notebook or learned something new!\n",
    "- https://www.kaggle.com/code/valentinwerner/915-deberta3base-training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8980926f",
   "metadata": {
    "papermill": {
     "duration": 0.005816,
     "end_time": "2024-04-08T01:35:01.337097",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.331281",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reserve 25% for models and system, then 25% for the public dataset.\n",
    "The other 50% is taken away for a test to minimize the risk of an OOM exception.\n",
    "Basically, with 75% of the data in the private set, it will be 3x the size of the public\n",
    "data set. A single model with the training set, with 16GB pulled out, is at 71% memory used which leaves just about the right amount that must suffice for the public test set.\n",
    "\n",
    "Reserving half the memory is probably overkill, given that the redesign to write the CSV file in batches of 50 documents and collect garbadge between the batches likely means the only linear growth will be in the initial Dataset object to feed the inferencing process (which remains done in one large batch of all json objects in the test set.) The rewrite seems to have little impact on wall time and so I will halt optimzation of the loops in favor of exploring inteferencing and ensemble based improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8a1a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T17:42:29.009801Z",
     "iopub.status.busy": "2024-03-10T17:42:29.009420Z",
     "iopub.status.idle": "2024-03-10T17:42:48.308737Z",
     "shell.execute_reply": "2024-03-10T17:42:48.307696Z",
     "shell.execute_reply.started": "2024-03-10T17:42:29.009771Z"
    },
    "papermill": {
     "duration": 0.005882,
     "end_time": "2024-04-08T01:35:01.349044",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.343162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "****# Comment out and make a Markup for contest submission\n",
    "#useToTestForPrivateSetSize = bytearray(int(20 * (1024 ** 3))) synatx error in case it is both uncommented and changed to a code block by accident\n",
    "\n",
    "## Initial parameters and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cc61e72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:01.362774Z",
     "iopub.status.busy": "2024-04-08T01:35:01.362437Z",
     "iopub.status.idle": "2024-04-08T01:35:01.373482Z",
     "shell.execute_reply": "2024-04-08T01:35:01.372716Z"
    },
    "papermill": {
     "duration": 0.020248,
     "end_time": "2024-04-08T01:35:01.375407",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.355159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "INFERENCE_MAX_LENGTH=2048 # 1024\n",
    "STRIDE=384\n",
    "\n",
    "# Note that training a model with stride, such as: https://www.kaggle.com/code/thedrcat/pii-data-detection-train-with-w-b\n",
    "# will also improve performance\n",
    "# the tokenizer will be taken from only the first model in a set of models\n",
    "# best choice is a model that differs only on the splits used.\n",
    "## model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-4702\"\n",
    "\n",
    "# Use for ensembles, and be sure to pin the needed version before running the code\n",
    "##model_path = \"/kaggle/input/piidd-extending-large-model-starter-model/output/checkpoint-4702\" # pin v12 LB=0.965 at tr.99 maxlen2048\n",
    "#model_path = \"/kaggle/input/piidd-extending-large-model-starter-trn2/output/checkpoint-367\" # xsmall with issues built with 10% of the data\n",
    "model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-13920\"\n",
    "\n",
    "# using the model path below to test a new trn2 bbuild with 1024 byte hidden/inference length (was 720)\n",
    "#model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-13188\"\n",
    "\n",
    "model_path2 = \"/kaggle/input/piidd-extending-large-model-starter-best2-trn2/output/model-trn2-v11-checkpoint-6594\"\n",
    "#model_path2 = \"/kaggle/input/piidd-extending-large-model-starter-best-trn2/output/model-trn2-v2-checkpoint-14106\"\n",
    "model_path3 = \"/kaggle/input/piidd-extending-large-model-starter-best2-trn2/output/model-trn2-v11-checkpoint-6594\" # avoiding 3 models due to time constraints when we must inference with 3x the public leaderboards data in 9 hours or less\n",
    "\n",
    "# Comment out when we have modified this for a single model run\n",
    "#model_path = \"/kaggle/input/piidd-extending-large-model-starter-model-trn2/output/checkpoint-14106\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84c8b7f3",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:01.388654Z",
     "iopub.status.busy": "2024-04-08T01:35:01.388402Z",
     "iopub.status.idle": "2024-04-08T01:35:19.313985Z",
     "shell.execute_reply": "2024-04-08T01:35:19.313204Z"
    },
    "papermill": {
     "duration": 17.934823,
     "end_time": "2024-04-08T01:35:19.316320",
     "exception": false,
     "start_time": "2024-04-08T01:35:01.381497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import argparse\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments, DataCollatorForTokenClassification\n",
    "from datasets import Dataset \n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "# Might revisit, but for now HF datasets module is not supported by the TPU / CUDA\n",
    "# I would need to rewrite without datasets to import Dataset\n",
    "# Comment out the import and device setting when using the GPU P100 or T4x2\n",
    "#import torch # we will use this to get some more GPU time from the TPUs\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# also, look for .to(device) and comment out those lines when not using the TPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e368a7",
   "metadata": {
    "papermill": {
     "duration": 0.006089,
     "end_time": "2024-04-08T01:35:19.329154",
     "exception": false,
     "start_time": "2024-04-08T01:35:19.323065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## â™Ÿï¸ Data Loading & Data Tokenization\n",
    "- This tokenizer is actually special, comparing to usual NLP challenges\n",
    "- inference tokenizer is a bit different than training tokenizer, because we don't have labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15efb6f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:19.342930Z",
     "iopub.status.busy": "2024-04-08T01:35:19.342282Z",
     "iopub.status.idle": "2024-04-08T01:35:19.349220Z",
     "shell.execute_reply": "2024-04-08T01:35:19.348484Z"
    },
    "papermill": {
     "duration": 0.016015,
     "end_time": "2024-04-08T01:35:19.351200",
     "exception": false,
     "start_time": "2024-04-08T01:35:19.335185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(example, tokenizer):\n",
    "    text = []\n",
    "    token_map = []\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    for t, ws in zip(example[\"tokens\"], example[\"trailing_whitespace\"]):\n",
    "        text.append(t)\n",
    "        token_map.extend([idx]*len(t))\n",
    "        if ws:\n",
    "            text.append(\" \")\n",
    "            token_map.append(-1)\n",
    "            \n",
    "        idx += 1\n",
    "        \n",
    "    tokenized = tokenizer(\"\".join(text), return_offsets_mapping=True, truncation=True, max_length=INFERENCE_MAX_LENGTH, stride=STRIDE, return_overflowing_tokens=True)\n",
    "        \n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"token_map\": token_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4fe72e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:19.365368Z",
     "iopub.status.busy": "2024-04-08T01:35:19.364846Z",
     "iopub.status.idle": "2024-04-08T01:35:20.674870Z",
     "shell.execute_reply": "2024-04-08T01:35:20.673869Z"
    },
    "papermill": {
     "duration": 1.319707,
     "end_time": "2024-04-08T01:35:20.677282",
     "exception": false,
     "start_time": "2024-04-08T01:35:19.357575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaaa8a7208040f78faa8b842b7a2668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f0667bd2fd41b0a48769018c2a8431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/5 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = json.load(open(\"/kaggle/input/pii-detection-removal-from-educational-data/test.json\"))\n",
    "\n",
    "ds = Dataset.from_dict({\n",
    "    \"full_text\": [x[\"full_text\"] for x in data],\n",
    "    \"document\": [x[\"document\"] for x in data],\n",
    "    \"tokens\": [x[\"tokens\"] for x in data],\n",
    "    \"trailing_whitespace\": [x[\"trailing_whitespace\"] for x in data],\n",
    "})\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "ds = ds.map(tokenize, fn_kwargs={\"tokenizer\": tokenizer}, num_proc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e05e484",
   "metadata": {
    "papermill": {
     "duration": 0.006701,
     "end_time": "2024-04-08T01:35:20.691508",
     "exception": false,
     "start_time": "2024-04-08T01:35:20.684807",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ðŸ‹ðŸ»â€â™€ï¸ Trainer Class based on the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "810400db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:20.707087Z",
     "iopub.status.busy": "2024-04-08T01:35:20.706289Z",
     "iopub.status.idle": "2024-04-08T01:35:23.235303Z",
     "shell.execute_reply": "2024-04-08T01:35:23.234503Z"
    },
    "papermill": {
     "duration": 2.539373,
     "end_time": "2024-04-08T01:35:23.237626",
     "exception": false,
     "start_time": "2024-04-08T01:35:20.698253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=args, \n",
    "    data_collator=collator, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6275ac6",
   "metadata": {
    "papermill": {
     "duration": 0.006507,
     "end_time": "2024-04-08T01:35:23.251263",
     "exception": false,
     "start_time": "2024-04-08T01:35:23.244756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This needs a second model build off the exact same LLM base (large in this case).\n",
    "It must be just another run as it must have the same tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "788be428",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:23.266708Z",
     "iopub.status.busy": "2024-04-08T01:35:23.266413Z",
     "iopub.status.idle": "2024-04-08T01:35:35.056478Z",
     "shell.execute_reply": "2024-04-08T01:35:35.055482Z"
    },
    "papermill": {
     "duration": 11.800206,
     "end_time": "2024-04-08T01:35:35.058934",
     "exception": false,
     "start_time": "2024-04-08T01:35:23.258728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model2 = AutoModelForTokenClassification.from_pretrained(model_path2)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator2 = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer2 = Trainer(\n",
    "    model=model2, \n",
    "    args=args, \n",
    "    data_collator=collator2, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33852d5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:35.074541Z",
     "iopub.status.busy": "2024-04-08T01:35:35.074226Z",
     "iopub.status.idle": "2024-04-08T01:35:36.379786Z",
     "shell.execute_reply": "2024-04-08T01:35:36.378995Z"
    },
    "papermill": {
     "duration": 1.315732,
     "end_time": "2024-04-08T01:35:36.382104",
     "exception": false,
     "start_time": "2024-04-08T01:35:35.066372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model3 = AutoModelForTokenClassification.from_pretrained(model_path3)\n",
    "\n",
    "# model.to(device) # used only with CUDA to access the TPU\n",
    "\n",
    "collator3 = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=16)\n",
    "args = TrainingArguments(\n",
    "    \".\", \n",
    "    per_device_eval_batch_size=1, \n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer3 = Trainer(\n",
    "    model=model3, \n",
    "    args=args, \n",
    "    data_collator=collator3, \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6cc303",
   "metadata": {
    "papermill": {
     "duration": 0.006778,
     "end_time": "2024-04-08T01:35:36.396455",
     "exception": false,
     "start_time": "2024-04-08T01:35:36.389677",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Striding functions\n",
    "\n",
    "As using the stride give an overlap in tokens, these have to be removed (either pick one side of the stride or average them, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c23eea0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:36.411638Z",
     "iopub.status.busy": "2024-04-08T01:35:36.410800Z",
     "iopub.status.idle": "2024-04-08T01:35:36.418373Z",
     "shell.execute_reply": "2024-04-08T01:35:36.417688Z"
    },
    "papermill": {
     "duration": 0.017221,
     "end_time": "2024-04-08T01:35:36.420371",
     "exception": false,
     "start_time": "2024-04-08T01:35:36.403150",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def backwards_map_preds(sub_predictions, max_len):\n",
    "    if max_len != 1: # nothing to map backwards if sequence is too short to be split in the first place\n",
    "        if i == 0:\n",
    "            # First sequence needs no SEP token (used to end a sequence)\n",
    "            sub_predictions = sub_predictions[:,:-1,:]\n",
    "        elif i == max_len-1:\n",
    "            # End sequence needs to CLS token + Stride tokens \n",
    "            sub_predictions = sub_predictions[:,1+STRIDE:,:] # CLS tokens + Stride tokens\n",
    "        else:\n",
    "            # Middle sequence needs to CLS token + Stride tokens + SEP token\n",
    "            sub_predictions = sub_predictions[:,1+STRIDE:-1,:]\n",
    "    return sub_predictions\n",
    "\n",
    "def backwards_map_(row_attribute, max_len):\n",
    "    # Same logics as for backwards_map_preds - except lists instead of 3darray\n",
    "    if max_len != 1:\n",
    "        if i == 0:\n",
    "            row_attribute = row_attribute[:-1]\n",
    "        elif i == max_len-1:\n",
    "            row_attribute = row_attribute[1+STRIDE:]\n",
    "        else:\n",
    "            row_attribute = row_attribute[1+STRIDE:-1]\n",
    "    return row_attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10c91943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:36.435042Z",
     "iopub.status.busy": "2024-04-08T01:35:36.434749Z",
     "iopub.status.idle": "2024-04-08T01:35:38.264486Z",
     "shell.execute_reply": "2024-04-08T01:35:38.263402Z"
    },
    "papermill": {
     "duration": 1.839393,
     "end_time": "2024-04-08T01:35:38.266569",
     "exception": false,
     "start_time": "2024-04-08T01:35:36.427176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing submission.csv with pauses for GC to work.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ,  0,7,4,B-NAME_STUDENT\n",
      "\n",
      "1 ,  1,7,6,B-NAME_STUDENT\n",
      "\n",
      "2 ,  2,7,9,B-NAME_STUDENT\n",
      "\n",
      "3 ,  3,7,10,I-NAME_STUDENT\n",
      "\n",
      "4 ,  4,7,482,B-NAME_STUDENT\n",
      "\n",
      "5 ,  5,7,483,I-NAME_STUDENT\n",
      "\n",
      "6 ,  6,7,738,B-NAME_STUDENT\n",
      "\n",
      "7 ,  7,7,741,B-NAME_STUDENT\n",
      "\n",
      "8 ,  8,7,742,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 ,  9,10,0,B-NAME_STUDENT\n",
      "\n",
      "10 ,  10,10,1,I-NAME_STUDENT\n",
      "\n",
      "11 ,  11,10,464,B-NAME_STUDENT\n",
      "\n",
      "12 ,  12,10,465,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 ,  13,16,4,B-NAME_STUDENT\n",
      "\n",
      "14 ,  14,16,5,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 ,  15,20,5,B-NAME_STUDENT\n",
      "\n",
      "16 ,  16,20,6,I-NAME_STUDENT\n",
      "\n",
      "17 ,  17,20,8,I-NAME_STUDENT\n",
      "\n",
      "18 ,  18,20,9,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 ,  19,56,12,B-NAME_STUDENT\n",
      "\n",
      "20 ,  20,56,13,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 ,  21,86,6,B-NAME_STUDENT\n",
      "\n",
      "22 ,  22,86,7,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 ,  23,93,0,B-NAME_STUDENT\n",
      "\n",
      "24 ,  24,93,1,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ,  25,104,7,B-NAME_STUDENT\n",
      "\n",
      "26 ,  26,104,8,B-NAME_STUDENT\n",
      "\n",
      "27 ,  27,104,9,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 ,  28,112,5,B-NAME_STUDENT\n",
      "\n",
      "29 ,  29,112,6,I-NAME_STUDENT\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ,  30,123,32,B-NAME_STUDENT\n",
      "\n",
      "31 ,  31,123,33,I-NAME_STUDENT\n",
      "\n",
      "32 ,  32,123,35,I-NAME_STUDENT\n",
      "\n",
      "33 ,  33,123,38,B-NAME_STUDENT\n",
      "\n",
      "34 ,  34,123,1509,B-URL_PERSONAL\n",
      "\n",
      "35 ,  35,123,1512,B-URL_PERSONAL\n",
      "\n",
      "36 ,  36,123,1514,B-URL_PERSONAL\n",
      "\n",
      "CPU times: user 1.33 s, sys: 117 ms, total: 1.44 s\n",
      "Wall time: 1.81 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "config = json.load(open(Path(model_path) / \"config.json\"))\n",
    "id2label = config[\"id2label\"]\n",
    "gcnow = 0\n",
    "giverowid = 0\n",
    "ansrow = 0\n",
    "\n",
    "print(\"Writing submission.csv with pauses for GC to work.\")\n",
    "with open(\"submission.csv\", 'w') as csv_file:\n",
    "    submission = pd.DataFrame(columns= [\"row_id\", \"document\", \"token\", \"label\"])\n",
    "    submission.to_csv(csv_file, header=True, index=False)\n",
    "\n",
    "    preds = []\n",
    "    \n",
    "    for row in ds:\n",
    "        if gcnow % 50 == 49:\n",
    "            _ = gc.collect()\n",
    "        gcnow += 1\n",
    "        \n",
    "        # keys that need to be re-assembled\n",
    "        row_preds = []\n",
    "        row_offset = []\n",
    "        ds_dict = {\n",
    "            \"document\":[],\n",
    "            \"token_map\":[],\n",
    "            \"offset_mapping\":[],\n",
    "            \"tokens\":[]\n",
    "        }\n",
    "        \n",
    "        # Build a dictionary for reassembling the predictions and call for the inference\n",
    "        for i, y in enumerate(row[\"offset_mapping\"]):\n",
    "            # create new datasset for each of of the splits per document\n",
    "            x = Dataset.from_dict({\n",
    "                \"token_type_ids\":[row[\"token_type_ids\"][i]],\n",
    "                \"input_ids\":[row[\"input_ids\"][i]],\n",
    "                \"attention_mask\":[row[\"attention_mask\"][i]],\n",
    "                \"offset_mapping\":[row[\"offset_mapping\"][i]]\n",
    "            })\n",
    "            # predict for that split\n",
    "            pred = trainer.predict(x).predictions\n",
    "            #pred = (trainer.predict(x).predictions + trainer2.predict(x).predictions)/2.0\n",
    "            #pred = 0.4 * trainer.predict(x).predictions + 0.3 * trainer2.predict(x).predictions + 0.4 * trainer3.predict(x).predictions\n",
    "            \n",
    "            # removing the stride and additional CLS & SEP that are created\n",
    "            row_preds.append(backwards_map_preds(pred, len(row[\"offset_mapping\"])))\n",
    "            row_offset += backwards_map_(y, len(row[\"offset_mapping\"]))\n",
    "   \n",
    "        # Complete the prediction\n",
    "        preds_final = []\n",
    "        p_concat = np.concatenate(row_preds, axis = 1)\n",
    "        predictions_softmax = np.exp(p_concat) / np.sum(np.exp(p_concat), axis = 2).reshape(p_concat.shape[0],p_concat.shape[1],1)\n",
    "        p_concat = p_concat.argmax(-1) # the highest probability class of PII predicted\n",
    "        predictions_without_O = predictions_softmax[:,:,:12].argmax(-1)\n",
    "        O_predictions = predictions_softmax[:,:,12] # the probability that it is not PII (aka class \"O\")\n",
    "        # this threshold seems more of an art than a science unless I can find a function to estimate or optimize it\n",
    "        threshold = 0.97 # 0.97 # 0.99 # the threshold of confidence below which we assume the prediction is a class of PII\n",
    "        preds_final.append(np.where(O_predictions < threshold, predictions_without_O , p_concat))\n",
    "\n",
    "        ds_dict[\"document\"].append(row[\"document\"])\n",
    "        ds_dict[\"tokens\"].append(row[\"tokens\"])\n",
    "        ds_dict[\"token_map\"].append(row[\"token_map\"])\n",
    "        ds_dict[\"offset_mapping\"].append(row_offset)\n",
    "        ds = Dataset.from_dict(ds_dict)\n",
    "        pairs = []\n",
    "        document, token, label, token_str = [], [], [], []\n",
    "\n",
    "        for p, token_map, offsets, tokens, doc in zip(preds_final, ds[\"token_map\"], ds[\"offset_mapping\"], ds[\"tokens\"], ds[\"document\"]):\n",
    "            for token_pred, (start_idx, end_idx) in zip(p[0], offsets):\n",
    "                label_pred = id2label[str(token_pred)]\n",
    "\n",
    "                if start_idx + end_idx == 0: continue\n",
    "\n",
    "                if token_map[start_idx] == -1:\n",
    "                    start_idx += 1\n",
    "\n",
    "                # ignore \"\\n\\n\"\n",
    "                while start_idx < len(token_map) and tokens[token_map[start_idx]].isspace():\n",
    "                    start_idx += 1\n",
    "\n",
    "                if start_idx >= len(token_map): break\n",
    "\n",
    "                token_id = token_map[start_idx]\n",
    "\n",
    "                # ignore \"O\" predictions and whitespace preds\n",
    "                if label_pred != \"O\" and token_id != -1:\n",
    "                    pair=(doc, token_id)\n",
    "                    if pair not in pairs:\n",
    "                        pairs.append(pair) # we will need to remember the pairing\n",
    "                        rowstr = str(giverowid)+\",\"+str(doc)+\",\"+str(token_id)+\",\"+label_pred+\"\\n\"\n",
    "                        print(ansrow, \", \", rowstr)\n",
    "                        ansrow += 1\n",
    "                        csv_file.write(rowstr)\n",
    "                        giverowid += 1\n",
    "        \n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c100551",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-08T01:35:38.287337Z",
     "iopub.status.busy": "2024-04-08T01:35:38.287024Z",
     "iopub.status.idle": "2024-04-08T01:35:38.294133Z",
     "shell.execute_reply": "2024-04-08T01:35:38.293163Z"
    },
    "papermill": {
     "duration": 0.019474,
     "end_time": "2024-04-08T01:35:38.295936",
     "exception": false,
     "start_time": "2024-04-08T01:35:38.276462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 31.36 GB\n",
      "Available RAM: 27.61 GB\n",
      "Used RAM: 3.28 GB\n",
      "Memory Usage: 11.9%\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get the memory details\n",
    "memory = psutil.virtual_memory()\n",
    "\n",
    "total_memory = memory.total / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Total RAM: {total_memory:.2f} GB\")\n",
    "\n",
    "available_memory = memory.available / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Available RAM: {available_memory:.2f} GB\")\n",
    "\n",
    "used_memory = memory.used / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Used RAM: {used_memory:.2f} GB\")\n",
    "\n",
    "memory_usage_percent = memory.percent\n",
    "print(f\"Memory Usage: {memory_usage_percent}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7500999,
     "sourceId": 66653,
     "sourceType": "competition"
    },
    {
     "datasetId": 4466098,
     "isSourceIdPinned": true,
     "sourceId": 7767497,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4601755,
     "sourceId": 7847820,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4735762,
     "sourceId": 8034032,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4568299,
     "isSourceIdPinned": true,
     "sourceId": 8056005,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170794644,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30636,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.575542,
   "end_time": "2024-04-08T01:35:41.392620",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-08T01:34:57.817078",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f97b617ffef4c769e47769ec69becb5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6d194e5556a74d35aec4acbb7824709c",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_fc76e852ccf74c99976e25395e5b6e31",
       "value": 5.0
      }
     },
     "27932a22d71b4275b04303fbc33feeff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a72222ee89241598383e0487bae2de0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4ba016dd5e714aecafcc536befce05d8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4fe61533365b45deaa427b2a6e4ea695": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_27932a22d71b4275b04303fbc33feeff",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_9be2021f18714aef8ce8175d2feb0628",
       "value": " 5/5 [00:00&lt;00:00, 56.59ex/s]"
      }
     },
     "50c83b264d51434fae1c244aab795d71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5aaaa8a7208040f78faa8b842b7a2668": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_80c7359358a34ad286083a9542d6e409",
        "IPY_MODEL_5b55bfec827b4842a0c228062a342cbd",
        "IPY_MODEL_4fe61533365b45deaa427b2a6e4ea695"
       ],
       "layout": "IPY_MODEL_ed8b128a998d477092aeadcd1653e151"
      }
     },
     "5b55bfec827b4842a0c228062a342cbd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7ba03af29e5e42a4b30c78bcd5861b1f",
       "max": 5.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_bba52e6679054303aa8756f6c7b4c0df",
       "value": 5.0
      }
     },
     "6d194e5556a74d35aec4acbb7824709c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7ba03af29e5e42a4b30c78bcd5861b1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "80c7359358a34ad286083a9542d6e409": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_50c83b264d51434fae1c244aab795d71",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_875c94c8802745a59d428f1c75c101c8",
       "value": "#0: 100%"
      }
     },
     "875c94c8802745a59d428f1c75c101c8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9351d547d26a4709a3849a6cc039ccd9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4ba016dd5e714aecafcc536befce05d8",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_b8b77a26be2d4017bdc3c5c8c0ab61bb",
       "value": "#1: 100%"
      }
     },
     "935818495de54bce9c57cf20556b05b0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9be2021f18714aef8ce8175d2feb0628": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8b77a26be2d4017bdc3c5c8c0ab61bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "bba52e6679054303aa8756f6c7b4c0df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "cca75f34fa094b8bb90f983a84e7be9d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e2df9cee8ace4993978b5e38efdc3067": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3a72222ee89241598383e0487bae2de0",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_cca75f34fa094b8bb90f983a84e7be9d",
       "value": " 5/5 [00:00&lt;00:00, 66.04ex/s]"
      }
     },
     "e6f0667bd2fd41b0a48769018c2a8431": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9351d547d26a4709a3849a6cc039ccd9",
        "IPY_MODEL_0f97b617ffef4c769e47769ec69becb5",
        "IPY_MODEL_e2df9cee8ace4993978b5e38efdc3067"
       ],
       "layout": "IPY_MODEL_935818495de54bce9c57cf20556b05b0"
      }
     },
     "ed8b128a998d477092aeadcd1653e151": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fc76e852ccf74c99976e25395e5b6e31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
